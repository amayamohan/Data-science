{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9e9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers , models # For building neural network models \n",
    "import matplotlib.pyplot as plt # For plotting graphs and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad0dbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d47bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset (MNIST dataset of handwritten digits)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8147ed8",
   "metadata": {},
   "source": [
    "X_train: images in the training set (usually 60,000 images), each is a 28x28 grayscale image.\n",
    "\n",
    "y_train: labels (correct digit 0-9) for each training image.\n",
    "\n",
    "X_test: images in the test set (usually 10,000 images) used to evaluate the model.\n",
    "\n",
    "y_test: labels for the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb88b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data\n",
    "X_train = X_train.astype(\"float32\") / 255.0   #astype(\"float32\") converts the image pixel values from integers to floating-point numbers.                                            \n",
    "X_test = X_test.astype(\"float32\") / 255.0     #Pixel values in MNIST images range from 0 to 255 (since each pixel is 8-bit grayscale).\n",
    "                                              #Dividing by 255.0 scales the values to the range 0.0 to 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604ed46f",
   "metadata": {},
   "source": [
    "The numbers like 0.0 are pixel brightness values scaled between 0 and 1.\n",
    "\n",
    "0.0 means the pixel is black (no brightness).\n",
    "\n",
    "Numbers closer to 1.0 mean brighter pixels (whiter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70038059",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3103d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the data to add a channel dimension (since CNNs expect 4D input: batch_size, height, width, channels)\n",
    "#-1 means: \"Keep the number of images (60000) automatically\" — you don’t have to specify 60000 explicitly.\n",
    "\n",
    "X_train = X_train.reshape(-1, 28,28,1)  \n",
    "X_test = X_test.reshape(-1, 28,28,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f49e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Build a simple CNN model\n",
    "\n",
    "#32 - number of filters in the convolutional layer (set 32 layers to learn 32 different features from the input images)\n",
    "#(3,3) - size of the filter (kernel)\n",
    "#activation='relu' - activation function to introduce non-linearity  - If the input is negative, output 0; if positive, output the same number.\n",
    "#input_shape=(28,28,1) - shape of the input images (28x28 pixels, 1 channel for grayscale)\n",
    "\n",
    "\n",
    "#MaxPooling2D((2,2)) - reduces the spatial dimensions (height and width) of the feature maps by a factor of 2\n",
    "model=models.Sequential([\n",
    "    layers.Conv2D(32,(3,3),activation='relu',input_shape=(28,28,1)),  #convolutional layer  \n",
    "    layers.MaxPooling2D((2,2)), #pooling layer\n",
    "    layers.Flatten(),            #flatten into ID\n",
    "    layers.Dense(64, activation='relu'), #fully connected layers\n",
    "    layers.Dense(10, activation='softmax')  #output layer(10 classes) (0-9 digits)(softmax for multi-class classification)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fcfb530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model\n",
    "\n",
    "\n",
    "# How to learn (optimizer)\n",
    "\n",
    "# What mistakes to measure (loss function)\n",
    "\n",
    "# How to check progress (metrics)\n",
    "#optimizer='adam' - optimization algorithm to adjust weights during training\n",
    "#loss='sparse_categorical_crossentropy' - loss function for multi-class classification with integer\n",
    "#metrics=['accuracy'] - metric to evaluate model performance during training and testing\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33a0b09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9979 - loss: 0.0064 - val_accuracy: 0.9838 - val_loss: 0.0550\n",
      "Epoch 2/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9984 - loss: 0.0057 - val_accuracy: 0.9847 - val_loss: 0.0573\n",
      "Epoch 3/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 0.9877 - val_loss: 0.0465\n",
      "Epoch 4/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9985 - loss: 0.0047 - val_accuracy: 0.9858 - val_loss: 0.0575\n",
      "Epoch 5/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9988 - loss: 0.0039 - val_accuracy: 0.9871 - val_loss: 0.0499\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64, #faster traning\n",
    "    validation_data=(X_test,y_test),\n",
    "    verbose=1  #shows progress bar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "939d872b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.71 %\n"
     ]
    }
   ],
   "source": [
    "#Evulate on test data\n",
    "test_loss,test_acc=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(\"Test Accuracy:\",round(test_acc * 100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ea0e723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction=model.predict(X_test[5:6])  #get prediction possibilities\n",
    "predicted_label=prediction.argmax()  #find the most likely class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a1f786e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(X_test[5].reshape(28,28),cmap=\"gray\")\n",
    "plt.title(\"prediction:\" + str(predicted_label))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
